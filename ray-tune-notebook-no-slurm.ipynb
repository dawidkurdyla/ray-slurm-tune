{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:30.759257Z",
     "iopub.status.busy": "2025-01-26T21:50:30.758941Z",
     "iopub.status.idle": "2025-01-26T21:50:30.764776Z",
     "shell.execute_reply": "2025-01-26T21:50:30.763986Z",
     "shell.execute_reply.started": "2025-01-26T21:50:30.759235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "from torch.optim import lr_scheduler\n",
    "from typing import Dict, Optional, Any\n",
    "import ray\n",
    "import time\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from tempfile import TemporaryDirectory\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:30.869320Z",
     "iopub.status.busy": "2025-01-26T21:50:30.869090Z",
     "iopub.status.idle": "2025-01-26T21:50:30.872591Z",
     "shell.execute_reply": "2025-01-26T21:50:30.871861Z",
     "shell.execute_reply.started": "2025-01-26T21:50:30.869301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:38:18.028348Z",
     "iopub.status.busy": "2025-01-26T21:38:18.028105Z",
     "iopub.status.idle": "2025-01-26T21:38:18.048414Z",
     "shell.execute_reply": "2025-01-26T21:38:18.047369Z",
     "shell.execute_reply.started": "2025-01-26T21:38:18.028328Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/kaggle/working/asl_train/N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-51232d93d59e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msource_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdest_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/working/asl_train/N'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "source_dir = <DIR_PATH_TO_ASL_TRAIN>\n",
    "root_dir = <ROOT_PROJECT_PATH>\n",
    "train_dir = os.path.join(root_dir,'asl_train')\n",
    "test_dir = os.path.join(root_dir,'asl_test')\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "for class_ in os.listdir(source_dir):\n",
    "    source_folder = os.path.join(source_dir,class_)\n",
    "    dest_folder = os.path.join(train_dir,class_)\n",
    "    os.mkdir(dest_folder)\n",
    "    \n",
    "    file_list = os.listdir(source_folder)\n",
    "    ceil_95 = int(len(file_list) * .95)\n",
    "    for file in file_list[:ceil_95]:\n",
    "        shutil.copy2(os.path.join(source_folder,file),os.path.join(dest_folder,file))\n",
    "    dest_folder_test = os.path.join(test_dir,class_)\n",
    "    os.mkdir(dest_folder_test)\n",
    "    for file in file_list[ceil_95:]:\n",
    "        shutil.copy2(os.path.join(source_folder,file),os.path.join(dest_folder_test,file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:33.030761Z",
     "iopub.status.busy": "2025-01-26T21:50:33.030466Z",
     "iopub.status.idle": "2025-01-26T21:50:33.037182Z",
     "shell.execute_reply": "2025-01-26T21:50:33.036208Z",
     "shell.execute_reply.started": "2025-01-26T21:50:33.030740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    test_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    full_dataset = datasets.ImageFolder(<DATA_TRAIN_PATH>)\n",
    "    test_dataset = datasets.ImageFolder(<DATA_TEST_PATH>,test_transform)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [0.95, 0.05])\n",
    "\n",
    "    train_dataset.dataset.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    val_dataset.dataset.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    val_loader =  torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    \n",
    "    class_names = full_dataset.classes\n",
    "\n",
    "    return train_loader, val_loader, test_loader,{'train': len(train_dataset), 'val': len(val_dataset)}, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:33.148447Z",
     "iopub.status.busy": "2025-01-26T21:50:33.148194Z",
     "iopub.status.idle": "2025-01-26T21:50:33.159736Z",
     "shell.execute_reply": "2025-01-26T21:50:33.158988Z",
     "shell.execute_reply.started": "2025-01-26T21:50:33.148426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    print('model loaded')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    train_loader, val_loader, _, dataset_sizes, class_names = load_data()\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    if config['normalization']:\n",
    "        model_ft.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),  \n",
    "            nn.Linear(num_ftrs, len(class_names))  \n",
    "        )\n",
    "    else:\n",
    "        model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_ft = nn.DataParallel(model_ft)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if config['optimizer'] == 'SGD':\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    elif config['optimizer'] == 'ADAM':\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=config['lr'], betas=(config['beta1'], config['beta2']), weight_decay=config['decay'])\n",
    "    else:\n",
    "        optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=config['lr'], alpha=config['alpha'], momentum=config['momentum'])\n",
    "    \n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=config['step_size'], gamma=config['gamma'])\n",
    "    since = time.time()\n",
    "    dataloaders = {'train' : train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "    if train.get_checkpoint():\n",
    "        loaded_checkpoint = train.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state, scheduler_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            model_ft.load_state_dict(model_state)\n",
    "            optimizer_ft.load_state_dict(optimizer_state)\n",
    "            exp_lr_scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        temp_checkpoint_dir = os.path.join(tempdir, 'checkpoint.pt')\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            print(f\"epoch: {epoch}\")\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model_ft.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model_ft.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                iterator = 0\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    if iterator % 100 == 0:\n",
    "                        print(f'Batch {iterator}/{len(dataloaders[phase])}')\n",
    "                    iterator += 1\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer_ft.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model_ft(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer_ft.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                if phase == 'train':\n",
    "                    exp_lr_scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                if phase == 'val':\n",
    "                    torch.save((model_ft.state_dict(),optimizer_ft.state_dict(), exp_lr_scheduler.state_dict()), temp_checkpoint_dir)    \n",
    "                    checkpoint = Checkpoint.from_directory(tempdir)\n",
    "                    print(f\"Val Loss: {epoch_loss}, Val Acc: {epoch_acc}\")\n",
    "                    train.report(\n",
    "                    {\"loss\": float(epoch_loss), \"accuracy\": float(epoch_acc)},\n",
    "                    checkpoint=checkpoint)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:33.275042Z",
     "iopub.status.busy": "2025-01-26T21:50:33.274815Z",
     "iopub.status.idle": "2025-01-26T21:50:33.281308Z",
     "shell.execute_reply": "2025-01-26T21:50:33.280549Z",
     "shell.execute_reply.started": "2025-01-26T21:50:33.275023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_best_model(best_result, smoke_test=False):\n",
    "    config = best_result.config\n",
    "    _, _, testloader, dataset_sizes, class_names = load_data()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_trained_model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    for param in best_trained_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = best_trained_model.fc.in_features\n",
    "    if config['normalization']:\n",
    "        best_trained_model.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),  \n",
    "            nn.Linear(num_ftrs, len(class_names))  \n",
    "        )\n",
    "    else:\n",
    "        best_trained_model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    \n",
    "    best_trained_model = best_trained_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state, scheduler_state = torch.load(checkpoint_path)\n",
    "    torch.save((model_state,optimizer_state, scheduler_state), 'model_state.pt') \n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = best_trained_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    print(\"Best trial test set accuracy: {}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:33.397421Z",
     "iopub.status.busy": "2025-01-26T21:50:33.397140Z",
     "iopub.status.idle": "2025-01-26T21:50:33.402683Z",
     "shell.execute_reply": "2025-01-26T21:50:33.401801Z",
     "shell.execute_reply.started": "2025-01-26T21:50:33.397371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def define_by_run_func(trial) -> Optional[Dict[str, Any]]:\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"ADAM\",'RMSprop'])\n",
    "\n",
    "    trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    if optimizer == \"SGD\":\n",
    "        trial.suggest_float(\"momentum\", 0, 0.9)\n",
    "    elif optimizer == 'ADAM':\n",
    "        trial.suggest_float(\"beta1\", .9, 0.999)\n",
    "        trial.suggest_float(\"beta2\", .9, 0.999)\n",
    "        trial.suggest_float(\"decay\", 0, .2)\n",
    "    else:\n",
    "        trial.suggest_float(\"momentum\", 0, 0.9)\n",
    "        trial.suggest_float('alpha',0.9,0.999)\n",
    "\n",
    "    trial.suggest_categorical(\"normalization\", [False, True])\n",
    "    trial.suggest_float('gamma',1e-3,1e-1)\n",
    "    trial.suggest_int(\"step_size\", 5, 15, step=2)\n",
    "        \n",
    "    return {\"epochs\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:33.520604Z",
     "iopub.status.busy": "2025-01-26T21:50:33.520336Z",
     "iopub.status.idle": "2025-01-26T21:50:35.750290Z",
     "shell.execute_reply": "2025-01-26T21:50:35.749436Z",
     "shell.execute_reply.started": "2025-01-26T21:50:33.520582Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "! wandb login <YOUR_WAND_API_KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:50:37.573273Z",
     "iopub.status.busy": "2025-01-26T21:50:37.572940Z",
     "iopub.status.idle": "2025-01-26T22:01:09.542312Z",
     "shell.execute_reply": "2025-01-26T22:01:09.541306Z",
     "shell.execute_reply.started": "2025-01-26T21:50:37.573246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-26 22:01:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:10:23.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.8/31.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 8.000: None | Iter 4.000: -0.7775895450145208 | Iter 2.000: -0.6055179090029041 | Iter 1.000: -0.6586398838334946<br>Logical resource usage: 2.0/4 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">   alpha</th><th style=\"text-align: right;\">   beta1</th><th style=\"text-align: right;\">  beta2</th><th style=\"text-align: right;\">    decay</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">     gamma</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th>normalization  </th><th>optimizer  </th><th style=\"text-align: right;\">  step_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_213866e6</td><td>TERMINATED</td><td>172.19.2.2:1372</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">0.993583</td><td style=\"text-align: right;\">0.94833</td><td style=\"text-align: right;\">0.0963138</td><td style=\"text-align: right;\">       4</td><td style=\"text-align: right;\">0.00799826</td><td style=\"text-align: right;\">1.79877e-05</td><td style=\"text-align: right;\">          </td><td>False          </td><td>ADAM       </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         615.671</td><td style=\"text-align: right;\">1.80127 </td><td style=\"text-align: right;\">  0.77759 </td></tr>\n",
       "<tr><td>train_model_bf99453e</td><td>TERMINATED</td><td>172.19.2.2:1413</td><td style=\"text-align: right;\">0.991016</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">       4</td><td style=\"text-align: right;\">0.0790526 </td><td style=\"text-align: right;\">0.000458005</td><td style=\"text-align: right;\">   0.80171</td><td>False          </td><td>RMSprop    </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         237.662</td><td style=\"text-align: right;\">0.204931</td><td style=\"text-align: right;\">  0.939739</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 21:50:37,621\tINFO wandb.py:319 -- Already logged into W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m model loaded\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m epoch: 0\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Currently logged in as: zogfryt. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Tracking run with wandb version 0.19.1\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-01-26_21-37-50_142043_40/artifacts/2025-01-26_21-50-37/train_model_2025-01-26_21-50-37/driver_artifacts/train_model_213866e6_1_beta1=0.9936,beta2=0.9483,decay=0.0963,epochs=4,gamma=0.0080,lr=0.0000,normalization=False,optimizer=ADAM,s_2025-01-26_21-50-37/wandb/run-20250126_215045-213866e6\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Syncing run train_model_213866e6\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/zogfryt/Sign%20language%20final2\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: 🚀 View run at https://wandb.ai/zogfryt/Sign%20language%20final2/runs/213866e6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1413)\u001b[0m model loaded\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m epoch: 0\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 0/1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Currently logged in as: zogfryt. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Tracking run with wandb version 0.19.1\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-01-26_21-37-50_142043_40/artifacts/2025-01-26_21-50-37/train_model_2025-01-26_21-50-37/driver_artifacts/train_model_bf99453e_2_alpha=0.9910,epochs=4,gamma=0.0791,lr=0.0005,momentum=0.8017,normalization=False,optimizer=RMSprop,step_siz_2025-01-26_21-50-42/wandb/run-20250126_215059-bf99453e\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Syncing run train_model_bf99453e\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/zogfryt/Sign%20language%20final2\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: 🚀 View run at https://wandb.ai/zogfryt/Sign%20language%20final2/runs/bf99453e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 100/1227\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 200/1227\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 300/1227\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 400/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 400/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 500/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 500/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 600/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 600/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 700/1227\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 800/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 800/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 900/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 900/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1000/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 1000/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1100/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 1100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/65\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Batch 0/65\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_model_2025-01-26_21-50-37/train_model_213866e6_1_beta1=0.9936,beta2=0.9483,decay=0.0963,epochs=4,gamma=0.0080,lr=0.0000,normalization=False,optimizer=ADAM,s_2025-01-26_21-50-37/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Val Loss: 2.801012395081423, Val Acc: 0.377541142303969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 21:54:43,590\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.513 s, which may be a performance bottleneck.\n",
      "2025-01-26 21:54:43,594\tWARNING util.py:201 -- The `process_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2025-01-26 21:54:43,596\tWARNING util.py:201 -- Processing trial results took 2.520 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-26 21:54:43,599\tWARNING util.py:201 -- The `process_trial_result` operation took 2.523 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m epoch: 1\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/1227\n",
      "\u001b[36m(train_model pid=1413)\u001b[0m Val Loss: 0.20493078955953142, Val Acc: 0.9397386253630203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1413)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_model_2025-01-26_21-50-37/train_model_bf99453e_2_alpha=0.9910,epochs=4,gamma=0.0791,lr=0.0005,momentum=0.8017,normalization=False,optimizer=RMSprop,step_siz_2025-01-26_21-50-42/checkpoint_000000)\n",
      "2025-01-26 21:54:52,726\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.569 s, which may be a performance bottleneck.\n",
      "2025-01-26 21:54:52,728\tWARNING util.py:201 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "2025-01-26 21:54:52,729\tWARNING util.py:201 -- Processing trial results took 2.573 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-26 21:54:52,730\tWARNING util.py:201 -- The `process_trial_result` operation took 2.575 s, which may be a performance bottleneck.\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                 accuracy ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                     loss ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                 accuracy 0.93974\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                     loss 0.20493\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:       time_since_restore 237.66169\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:         time_this_iter_s 237.66169\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:             time_total_s 237.66169\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:                timestamp 1737928490\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: 🚀 View run train_model_bf99453e at: https://wandb.ai/zogfryt/Sign%20language%20final2/runs/bf99453e\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/zogfryt/Sign%20language%20final2\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=1547)\u001b[0m wandb: Find logs at: ./wandb/run-20250126_215059-bf99453e/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 300/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 400/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 500/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 600/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 700/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 800/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 900/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1000/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/65\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Val Loss: 2.352248901780785, Val Acc: 0.6055179090029041\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_model_2025-01-26_21-50-37/train_model_213866e6_1_beta1=0.9936,beta2=0.9483,decay=0.0963,epochs=4,gamma=0.0080,lr=0.0000,normalization=False,optimizer=ADAM,s_2025-01-26_21-50-37/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 300/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 400/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 500/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 600/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 700/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 800/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 900/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1000/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_model_2025-01-26_21-50-37/train_model_213866e6_1_beta1=0.9936,beta2=0.9483,decay=0.0963,epochs=4,gamma=0.0080,lr=0.0000,normalization=False,optimizer=ADAM,s_2025-01-26_21-50-37/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Val Loss: 2.0326963865053043, Val Acc: 0.7166021297192643\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m epoch: 3\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 300/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 400/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 500/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 600/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 700/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 800/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 900/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1000/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1100/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 1200/1227\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Batch 0/65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 22:01:00,740\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_model_2025-01-26_21-50-37' in 0.0105s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Val Loss: 1.801271633164811, Val Acc: 0.7775895450145208\n",
      "\u001b[36m(train_model pid=1372)\u001b[0m Training complete in 10m 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=1372)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_model_2025-01-26_21-50-37/train_model_213866e6_1_beta1=0.9936,beta2=0.9483,decay=0.0963,epochs=4,gamma=0.0080,lr=0.0000,normalization=False,optimizer=ADAM,s_2025-01-26_21-50-37/checkpoint_000003)\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                                                                                \n",
      "2025-01-26 22:01:02,235\tINFO tune.py:1041 -- Total run time: 624.64 seconds (623.11 seconds for the tuning loop).\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                 accuracy ▁▅▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                     loss █▅▃▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                 accuracy 0.77759\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                     loss 1.80127\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:       time_since_restore 615.67074\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:         time_this_iter_s 124.66966\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:             time_total_s 615.67074\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:                timestamp 1737928860\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: 🚀 View run train_model_213866e6 at: https://wandb.ai/zogfryt/Sign%20language%20final2/runs/213866e6\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/zogfryt/Sign%20language%20final2\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=1411)\u001b[0m wandb: Find logs at: ./wandb/run-20250126_215045-213866e6/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'optimizer': 'RMSprop', 'lr': 0.00045800507405258556, 'momentum': 0.8017096272982344, 'alpha': 0.9910161904431056, 'normalization': False, 'gamma': 0.07905262424749769, 'step_size': 9, 'epochs': 4}\n",
      "Best trial final validation loss: 0.20493078955953142\n",
      "Best trial final validation accuracy: 0.9397386253630203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-86cf3bb68433>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state, optimizer_state, scheduler_state = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.10758620689655173\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2, smoke_test=False):\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "        metric='accuracy',\n",
    "        mode='min')\n",
    "\n",
    "    algo = OptunaSearch(space=define_by_run_func,metric=\"accuracy\", mode=\"min\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=2)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            search_alg=algo,\n",
    "            num_samples=num_samples\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "            callbacks=[WandbLoggerCallback(project=\"Sign language final2\")]\n",
    "        )\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result, smoke_test=smoke_test)\n",
    "\n",
    "main(num_samples=2, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T15:49:00.661919Z",
     "iopub.status.idle": "2025-01-10T15:49:00.662196Z",
     "shell.execute_reply": "2025-01-10T15:49:00.662084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "setup_wandb(config, project=\"Sign language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 23079,
     "sourceId": 29550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
